
# [Paper](https://arxiv.org/pdf/1907.09658v7.pdf) [[code](https://github.com/fandulu/DD-Net)]
**Title**   :   Make Skeleton-based Action Recognition ModelSmaller, Faster and Better  

**Author**  : Fan Yang, Sakriani Sakti1, Yang Wu, Satoshi Nakamura

**From**   :   Nara Institute of Science and Technology, Japan  
&emsp;&emsp;&emsp;&nbsp;RIKEN, AIP, Japan,  
&emsp;&emsp;&emsp;&nbsp;Kyoto University, Japan  
**Year**  :   2019

# Details
## Summary
|![fig_01](Images/DD_Net/fig_01.png)|
|:--:| 
| *Fig. 1: Examples of skeleton sequence properties* |
* Want to tackle various skeleton properties shown in figure 1
* Introduce Joint Collection Distances (JCD) to tackle viewpoint variation
* Lightweight structure (0.15 million parameters)
* ~3500 FPS on GTX1080Ti
* ~2000 FPS on Intel E5-2620

|![fig_02](Images/DD_Net/fig_02.png)|
|:--:| 
| *Fig. 2: The network architecture of DD-Net. “2×CNN(3, 2*filters), /2” denotes two 1D ConvNet layers (kernel size = 3, channels = 2*filters) and a Maxpooling (strides = 2). GAP denotes Global Average Pooling* |
##  Modeling Location-viewpoint Invariant Feature by Joint Collection Distances (JCD) 
|![fig_03](Images/DD_Net/fig_03.png)|
|:--:| 
| *Fig. 3: An example of Joint Collection Distances (JCD) feature at frame k, where the number of joints is N* |
* Calculate the Euclidean distances between a pair of collective joints to obtain a symmetric matrix
* Use half to remove redundancy
* JCD matrix is flatten into a 1D vector as the model's input

##  Modeling Global Scale-invariant Motions by a Two-scale Motion Feature
* JCD feature is location viewpoint invariant, but it does not contain global motion information (Fig.1 (c))
* Idea is inspired by [SlowFast](https://arxiv.org/pdf/1812.03982.pdf)
* Two-scale motions can be generated by the following equation: <img src="Images/DD_Net/formula_02.png" height="70px">
    * where *M* denotes the slow motion and fast motion at frame *k* respectively
    * *S<sup>k+1</sup>* and *S<sup>k+2</sup>* are behind *S<sup>k</sup>* for one frame and two frames respectively
* Corresponding to *S<sup>[1,...,K]</sup>*, we have *M<sup>[1,...,K−1]</sup>* slow and *M<sup>[1,...,K/2−1]</sup>* fast
* Flatten to 1D vector
* To match JCD feature, perform linear interpolation to resize *M<sup>[1,...,K−1]</sup>* slow and *M<sup>[1,...,K/2−1]</sup>* fast to *M<sup>[1,...,K]</sup>* slow and *M<sup>[1,...,K/2]</sup>* fast

##  Modeling Joint Correlations by an Embedding
* The correlation of joints is automatically learned through the embedding as depicted in figure 2
* As another beneﬁt, the embedding process also reduces the effect of skeleton noise

## Experiments
* Train and test on 2 datasets:
    * [SHREC17](https://shapenet.cs.stanford.edu/shrec17/)
    * [JHMDB](http://jhmdb.is.tue.mpg.de/)

|![table_01](Images/DD_Net/table_01.png)|
|:--:| 
| *Table I: Properties of experimental datasets* |

## Training Details
* Adam (*β*<sub>1</sub> = 0.9, *β*<sub>2</sub> = 0.999)
* Annealing learning rate that drops from 1e<sup>-3</sup> to 1e<sup>−5</sup>

## SHREC17
|![table_02](Images/DD_Net/table_02.png)|
|:--:| 
| *TABLE II: Results on SHREC (Using 3D skeletons only)* |

## JHMDB
|![table_03](Images/DD_Net/table_03.png)|
|:--:| 
| *TABLE III: Results on JHMDB (Using 2D skeletons only)* |
